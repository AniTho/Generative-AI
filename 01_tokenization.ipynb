{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Objective\n",
    "\n",
    "The objective of the notebook is to study and implement various tokenizers that are prominent in the literature. Implemented two tokenizers:\n",
    "\n",
    "- Simple Regex Based Tokenizer\n",
    "- BPE (Byte Pair encoding) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import regex as re\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset_path = pathlib.Path('./datasets')\n",
    "        self.dataset_file = 'the-verdict.txt'\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the data: 20479\n",
      "Total number of unique characters in the data: 62\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      " \"The height of his glory\"--that was what the women called it. I can hear\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(cfg.dataset_path, cfg.dataset_file), 'r') as file:\n",
    "    # Read but ignore lines with only new line character\n",
    "    data = [line for line in file.readlines() if line != '\\n']\n",
    "\n",
    "raw_data = ' '.join(data)\n",
    "print(f'Total number of characters in the data: {len(raw_data)}')\n",
    "print(f'Total number of unique characters in the data: {len(set(raw_data))}')\n",
    "print(raw_data[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex based tokenizer\n",
    "\n",
    "Regex based tokenizer involves finding a regular expression that can effectively convert your raw data into tokens. The common idea is to split your raw data into words where words are found out by splitting based on some heurestics like splitting based on spaces, tabs, new line, exclamation point. By extracting unique words from your split raw text, a vocabulary of words/tokens is build that is then assigned a unique integer that is used to map words/tokens to token idx.\n",
    "\n",
    "It is a design based tokenizer where domain knowledge and the required task that is required to split the raw data. For eg. If working with code generator tabs and spaces can't be neglected and will require special tokens in vocabulary for them. \n",
    "\n",
    "__NOTE:__ Also special tokens (like <unk> or |unk|) might be required in case some word that is not present in vocabulary is encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(raw_text, split_expression):\n",
    "    return re.split(split_expression, raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that,', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory,', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting,', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow,', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera.', ' ', '(Though', ' ', 'I', ' ', 'rather', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'have', ' ', 'been', ' ', 'Rome', ' ', 'or', ' ', 'Florence.)', '\\n ', '\"The', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory\"--that', ' ', 'was', ' ', 'what', ' ', 'the', ' ', 'women', ' ', 'called', ' ', 'it.', ' ', 'I', ' ', 'can', ' ', 'hear', ' ', 'Mrs.', ' ', 'Gideon', ' ', 'Thwing--his', ' ', 'last', ' ', 'Chicago', ' ', 'sitter--deploring', ' ', 'his', ' ', 'unaccountable', ' ', 'abdication.', ' ', '\"Of', ' ', 'course', ' ', \"it's\", ' ', 'going', ' ', 'to', ' ', 'send', ' ', 'the', ' ', 'value', ' ', 'of', ' ', 'my', ' ', 'picture', ' ', \"'way\", ' ', 'up;', ' ', 'but', ' ', 'I', ' ', \"don't\", ' ', 'think', ' ', 'of', ' ']\n",
      "Number of tokens: 7267\n"
     ]
    }
   ],
   "source": [
    "split_expression = r'(\\s+)' # split based on spaces\n",
    "tokens = text_to_tokens(raw_data, split_expression)\n",
    "print(tokens[:200])\n",
    "print(f'Number of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that,', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory,', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting,', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow,', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera.', ' ', '(Though', ' ', 'I', ' ', 'rather', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'have', ' ', 'been', ' ', 'Rome', ' ', 'or', ' ', 'Florence.)', '\\n ', '\"The', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory\"--that', ' ', 'was', ' ', 'what', ' ', 'the', ' ', 'women', ' ', 'called', ' ', 'it.', ' ', 'I', ' ', 'can', ' ', 'hear', ' ', 'Mrs.', ' ', 'Gideon', ' ', 'Thwing--his', ' ', 'last', ' ', 'Chicago', ' ', 'sitter--deploring', ' ', 'his', ' ', 'unaccountable', ' ', 'abdication.', ' ', '\"Of', ' ', 'course', ' ', \"it's\", ' ', 'going', ' ', 'to', ' ', 'send', ' ', 'the', ' ', 'value', ' ', 'of', ' ', 'my', ' ', 'picture', ' ', \"'way\", ' ', 'up;', ' ', 'but', ' ', 'I', ' ', \"don't\", ' ', 'think', ' ', 'of', ' ']\n",
      "Number of tokens: 7267\n"
     ]
    }
   ],
   "source": [
    "split_expression = r'(\\s+|\\n)' # split based on spaces and new line\n",
    "tokens = text_to_tokens(raw_data, split_expression)\n",
    "print(tokens[:200])\n",
    "print(f'Number of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '-', '', '-', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '-', '', '-', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that,', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory,', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting,', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow,', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera.', ' ', '(Though', ' ', 'I', ' ', 'rather', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'have', ' ', 'been', ' ', 'Rome', ' ', 'or', ' ', 'Florence.)', '\\n ', '\"The', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory\"', '-', '', '-', 'that', ' ', 'was', ' ', 'what', ' ', 'the', ' ', 'women', ' ', 'called', ' ', 'it.', ' ', 'I', ' ', 'can', ' ', 'hear', ' ', 'Mrs.', ' ', 'Gideon', ' ', 'Thwing', '-', '', '-', 'his', ' ', 'last', ' ', 'Chicago', ' ', 'sitter', '-', '', '-', 'deploring', ' ', 'his', ' ', 'unaccountable', ' ', 'abdication.', ' ', '\"Of', ' ', 'course', ' ', \"it's\", ' ', 'going', ' ', 'to', ' ', 'send', ' ', 'the', ' ', 'value', ' ']\n",
      "Number of tokens: 7731\n"
     ]
    }
   ],
   "source": [
    "split_expression = r'(-|--|---|\\s+|\\n)' # split based on spaces, new line and dashes (em dash, en dash, hyphen)\n",
    "tokens = text_to_tokens(raw_data, split_expression)\n",
    "print(tokens[:200])\n",
    "print(f'Number of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '-', '', '-', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '-', '', '-', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera', '.', '', ' ', '', '(', 'Though', ' ', 'I', ' ', 'rather', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'have', ' ', 'been', ' ', 'Rome', ' ', 'or', ' ', 'Florence', '.', '', ')', '', '\\n ', '', '\"', 'The', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', '\"', '', '-', '', '-', 'that', ' ', 'was', ' ', 'what', ' ', 'the', ' ', 'women', ' ', 'called', ' ', 'it', '.', '', ' ', 'I', ' ', 'can', ' ', 'hear', ' ', 'Mrs', '.', '', ' ', 'Gideon', ' ', 'Thwing', '-', '', '-', 'his', ' ', 'last', ' ', 'Chicago', ' ', 'sitter', '-', '', '-']\n",
      "Number of tokens: 9341\n"
     ]
    }
   ],
   "source": [
    "split_expression = r'([,.:;?!_\\(\\)\\\"\\']|-|--|---|\\s+|\\n)' # Including punctuations as well\n",
    "tokens = text_to_tokens(raw_data, split_expression)\n",
    "print(tokens[:200])\n",
    "print(f'Number of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '-', '-', 'though', 'a', 'good', 'fellow', 'enough', '-', '-', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and']\n",
      "Number of tokens: 4863\n"
     ]
    }
   ],
   "source": [
    "# Dropping tokens with only spaces, other pronounciation add some context as well\n",
    "tokens_cleaned = [token.strip() for token in tokens if token.strip()]\n",
    "print(tokens_cleaned[:50])\n",
    "print(f'Number of tokens: {len(tokens_cleaned)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n",
      "Number of tokens: 4765\n"
     ]
    }
   ],
   "source": [
    "# combining dashes if consecutive for em and en dash\n",
    "final_tokens = []\n",
    "i = 0\n",
    "while i < (len(tokens_cleaned) - 1):\n",
    "    if tokens_cleaned[i] == '-' and tokens_cleaned[i+1] == '-':\n",
    "        if i+2 < len(tokens_cleaned) and tokens_cleaned[i+2] == '-':\n",
    "            final_tokens.append('---')\n",
    "            i = i+2\n",
    "        else:\n",
    "            final_tokens.append('--')\n",
    "            i = i+1\n",
    "    else:\n",
    "        final_tokens.append(tokens_cleaned[i])\n",
    "    i = i+1\n",
    "print(final_tokens[:50])\n",
    "print(f'Number of tokens: {len(final_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding/Decoding text to token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 1140\n",
      "['yet', 'you', 'younger', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(final_tokens))\n",
    "print(f'Number of unique tokens: {len(vocab)}')\n",
    "print(vocab[-5:])\n",
    "tokens_to_idx = {token:idx for idx, token in enumerate(vocab)}\n",
    "tokens_to_idx['|unk|'] = len(tokens_to_idx)\n",
    "idx_to_token = {idx:token for token, idx in tokens_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_split_processing(tokens):\n",
    "    tokens_cleaned = [token.strip() for token in tokens if token.strip()]\n",
    "    final_tokens = []\n",
    "    i = 0\n",
    "    while i < (len(tokens_cleaned) - 1):\n",
    "        if tokens_cleaned[i] == '-' and tokens_cleaned[i+1] == '-':\n",
    "            if i+2 < len(tokens_cleaned) and tokens_cleaned[i+2] == '-':\n",
    "                final_tokens.append('---')\n",
    "                i = i+2\n",
    "            else:\n",
    "                final_tokens.append('--')\n",
    "                i = i+1\n",
    "        else:\n",
    "            final_tokens.append(tokens_cleaned[i])\n",
    "        i = i+1\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack']\n",
      "[54, 45, 150, 1013, 58]\n",
      "I HAD always thought Jack\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I HAD always thought Jack Gisburn' # Test Sentence\n",
    "final_split_expression= r'([,.:;?_\\(\\)\\\"\\']|-|--|---|\\s+|\\n)'\n",
    "sentence_tokens = post_split_processing(text_to_tokens(sentence, final_split_expression))\n",
    "print(sentence_tokens)\n",
    "sentence_tokens = [tokens_to_idx[word_token] for word_token in sentence_tokens] # Doesn't handle if token is not present\n",
    "print(sentence_tokens)\n",
    "decoded_sentence = ' '.join([idx_to_token[token_idx] for token_idx in sentence_tokens])\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Andrew']\n",
      "[69, 1140, 588, 1140]\n",
      "My |unk| is |unk|\n"
     ]
    }
   ],
   "source": [
    "sentence = 'My name is Andrew.' # Test Sentence\n",
    "final_split_expression= r'([,.:;?_\\(\\)\\\"\\']|-|--|---|\\s+|\\n)'\n",
    "sentence_tokens = post_split_processing(text_to_tokens(sentence, final_split_expression))\n",
    "print(sentence_tokens)\n",
    "sentence_tokens = [tokens_to_idx.get(word_token, tokens_to_idx['|unk|']) for word_token in sentence_tokens]\n",
    "print(sentence_tokens)\n",
    "decoded_sentence = ' '.join([idx_to_token[token_idx] for token_idx in sentence_tokens])\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying our tokenizer class functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Build Vocabulary ##########\n"
     ]
    }
   ],
   "source": [
    "from tokenizer.simple_tokenizer import RegexTokenizer\n",
    "split_regex= r'([,.:;?!_\\(\\)\\\"\\']|-|--|---|\\s+|\\n)'\n",
    "tokenizer = RegexTokenizer(raw_data, split_regex, special_tokens={'|unk|', }, unknown_token='|unk|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 1140, 588, 1140]\n",
      "My |unk| is |unk|\n"
     ]
    }
   ],
   "source": [
    "sentence = 'My name is Andrew.'\n",
    "encoded_sentence = tokenizer.encode(sentence)\n",
    "print(encoded_sentence)\n",
    "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
