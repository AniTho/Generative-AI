{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Objective\n",
    "\n",
    "The objective of the notebook is to study and implement various tokenizers that are prominent in the literature. Implemented two tokenizers:\n",
    "\n",
    "- Simple Regex Based Tokenizer __(WORD LEVEL)__\n",
    "  \n",
    "    This tokenizer splits the input text into tokens based on simple regular expressions, typically at word boundaries, such as spaces, punctuation marks, and special characters. It requires domain knowledge to effectively convert raw text into vocabulary (which is rule based / heurestic driven)\n",
    "\n",
    "- BPE (Byte Pair encoding) Tokenizer __(SUBWORD LEVEL)__\n",
    "\n",
    "    This tokenizer operates at the subword level and uses an algorithm to iteratively merge the most frequent pairs of bytes (or characters) in the text. This technique helps to break down words into smaller, more manageable subword units, improving the tokenizer's ability to handle out-of-vocabulary words and languages with rich morphology\n",
    "\n",
    "\n",
    "There is also an additional tokenizer that is used which is character tokenizer which breaks the input text into individual characters, treating each character as a token. Character tokenization is useful for languages with complex morphology or in cases where fine-grained tokenization is needed, such as in character-level language models or spelling correction tasks but it also adds an additional overhead since more number of tokens is required to represent same sentence but the deep learning models built subsequently can required huge resource if the length of sentence is too long. This notebook doesn't have character tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import regex as re\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.dataset_path = pathlib.Path('./datasets')\n",
    "        self.dataset_file = 'the-verdict.txt'\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the data: 20479\n",
      "Total number of unique characters in the data: 62\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      " \"The height of his glory\"--that was what the women called it. I can hear\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(cfg.dataset_path, cfg.dataset_file), 'r') as file:\n",
    "    # Read but ignore lines with only new line character\n",
    "    data = [line for line in file.readlines() if line != '\\n']\n",
    "\n",
    "raw_data = ' '.join(data)\n",
    "print(f'Total number of characters in the data: {len(raw_data)}')\n",
    "print(f'Total number of unique characters in the data: {len(set(raw_data))}')\n",
    "print(raw_data[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex based tokenizer\n",
    "\n",
    "Regex based tokenizer involves finding a regular expression that can effectively convert your raw data into tokens. The common idea is to split your raw data into words where words are found out by splitting based on some heurestics like splitting based on spaces, tabs, new line, exclamation point. By extracting unique words from your split raw text, a vocabulary of words/tokens is build that is then assigned a unique integer that is used to map words/tokens to token idx.\n",
    "\n",
    "It is a design based tokenizer where domain knowledge and the required task that is required to split the raw data. For eg. If working with code generator tabs and spaces can't be neglected and will require special tokens in vocabulary for them. \n",
    "\n",
    "__NOTE:__ Also special tokens (like <unk> or |unk|) might be required in case some word that is not present in vocabulary is encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(raw_text, split_expression):\n",
    "    return re.split(split_expression, raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that,', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory,', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting,', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow,', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera.', ' ', '(Though', ' ', 'I', ' ', 'rather', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'have', ' ', 'been', ' ', 'Rome', ' ', 'or', ' ', 'Florence.)', '\\n ', '\"The', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory\"--that', ' ', 'was', ' ', 'what', ' ', 'the', ' ', 'women', ' ', 'called', ' ', 'it.', ' ', 'I', ' ', 'can', ' ', 'hear', ' ', 'Mrs.', ' ', 'Gideon', ' ', 'Thwing--his', ' ', 'last', ' ', 'Chicago', ' ', 'sitter--deploring', ' ', 'his', ' ', 'unaccountable', ' ', 'abdication.', ' ', '\"Of', ' ', 'course', ' ', \"it's\", ' ', 'going', ' ', 'to', ' ', 'send', ' ', 'the', ' ', 'value', ' ', 'of', ' ', 'my', ' ', 'picture', ' ', \"'way\", ' ', 'up;', ' ', 'but', ' ', 'I', ' ', \"don't\", ' ', 'think', ' ', 'of', ' ']\n",
      "Number of tokens: 7267\n"
     ]
    }
   ],
   "source": [
    "split_expression = r'(\\s+)' # split based on spaces\n",
    "tokens = text_to_tokens(raw_data, split_expression)\n",
    "print(tokens[:200])\n",
    "print(f'Number of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that,', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory,', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting,', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow,', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera.', ' ', '(Though', ' ', 'I', ' ', 'rather', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'have', ' ', 'been', ' ', 'Rome', ' ', 'or', ' ', 'Florence.)', '\\n ', '\"The', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory\"--that', ' ', 'was', ' ', 'what', ' ', 'the', ' ', 'women', ' ', 'called', ' ', 'it.', ' ', 'I', ' ', 'can', ' ', 'hear', ' ', 'Mrs.', ' ', 'Gideon', ' ', 'Thwing--his', ' ', 'last', ' ', 'Chicago', ' ', 'sitter--deploring', ' ', 'his', ' ', 'unaccountable', ' ', 'abdication.', ' ', '\"Of', ' ', 'course', ' ', \"it's\", ' ', 'going', ' ', 'to', ' ', 'send', ' ', 'the', ' ', 'value', ' ', 'of', ' ', 'my', ' ', 'picture', ' ', \"'way\", ' ', 'up;', ' ', 'but', ' ', 'I', ' ', \"don't\", ' ', 'think', ' ', 'of', ' ']\n",
      "Number of tokens: 7267\n"
     ]
    }
   ],
   "source": [
    "split_expression = r'(\\s+|\\n)' # split based on spaces and new line\n",
    "tokens = text_to_tokens(raw_data, split_expression)\n",
    "print(tokens[:200])\n",
    "print(f'Number of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '-', '', '-', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '-', '', '-', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that,', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory,', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting,', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow,', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera.', ' ', '(Though', ' ', 'I', ' ', 'rather', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'have', ' ', 'been', ' ', 'Rome', ' ', 'or', ' ', 'Florence.)', '\\n ', '\"The', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory\"', '-', '', '-', 'that', ' ', 'was', ' ', 'what', ' ', 'the', ' ', 'women', ' ', 'called', ' ', 'it.', ' ', 'I', ' ', 'can', ' ', 'hear', ' ', 'Mrs.', ' ', 'Gideon', ' ', 'Thwing', '-', '', '-', 'his', ' ', 'last', ' ', 'Chicago', ' ', 'sitter', '-', '', '-', 'deploring', ' ', 'his', ' ', 'unaccountable', ' ', 'abdication.', ' ', '\"Of', ' ', 'course', ' ', \"it's\", ' ', 'going', ' ', 'to', ' ', 'send', ' ', 'the', ' ', 'value', ' ']\n",
      "Number of tokens: 7731\n"
     ]
    }
   ],
   "source": [
    "split_expression = r'(-|--|---|\\s+|\\n)' # split based on spaces, new line and dashes (em dash, en dash, hyphen)\n",
    "tokens = text_to_tokens(raw_data, split_expression)\n",
    "print(tokens[:200])\n",
    "print(f'Number of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '-', '', '-', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '-', '', '-', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera', '.', '', ' ', '', '(', 'Though', ' ', 'I', ' ', 'rather', ' ', 'thought', ' ', 'it', ' ', 'would', ' ', 'have', ' ', 'been', ' ', 'Rome', ' ', 'or', ' ', 'Florence', '.', '', ')', '', '\\n ', '', '\"', 'The', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', '\"', '', '-', '', '-', 'that', ' ', 'was', ' ', 'what', ' ', 'the', ' ', 'women', ' ', 'called', ' ', 'it', '.', '', ' ', 'I', ' ', 'can', ' ', 'hear', ' ', 'Mrs', '.', '', ' ', 'Gideon', ' ', 'Thwing', '-', '', '-', 'his', ' ', 'last', ' ', 'Chicago', ' ', 'sitter', '-', '', '-']\n",
      "Number of tokens: 9341\n"
     ]
    }
   ],
   "source": [
    "split_expression = r'([,.:;?!_\\(\\)\\\"\\']|-|--|---|\\s+|\\n)' # Including punctuations as well\n",
    "tokens = text_to_tokens(raw_data, split_expression)\n",
    "print(tokens[:200])\n",
    "print(f'Number of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '-', '-', 'though', 'a', 'good', 'fellow', 'enough', '-', '-', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and']\n",
      "Number of tokens: 4863\n"
     ]
    }
   ],
   "source": [
    "# Dropping tokens with only spaces, other pronounciation add some context as well\n",
    "tokens_cleaned = [token.strip() for token in tokens if token.strip()]\n",
    "print(tokens_cleaned[:50])\n",
    "print(f'Number of tokens: {len(tokens_cleaned)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n",
      "Number of tokens: 4765\n"
     ]
    }
   ],
   "source": [
    "# combining dashes if consecutive for em and en dash\n",
    "final_tokens = []\n",
    "i = 0\n",
    "while i < (len(tokens_cleaned) - 1):\n",
    "    if tokens_cleaned[i] == '-' and tokens_cleaned[i+1] == '-':\n",
    "        if i+2 < len(tokens_cleaned) and tokens_cleaned[i+2] == '-':\n",
    "            final_tokens.append('---')\n",
    "            i = i+2\n",
    "        else:\n",
    "            final_tokens.append('--')\n",
    "            i = i+1\n",
    "    else:\n",
    "        final_tokens.append(tokens_cleaned[i])\n",
    "    i = i+1\n",
    "print(final_tokens[:50])\n",
    "print(f'Number of tokens: {len(final_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding/Decoding text to token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 1140\n",
      "['yet', 'you', 'younger', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(final_tokens))\n",
    "print(f'Number of unique tokens: {len(vocab)}')\n",
    "print(vocab[-5:])\n",
    "tokens_to_idx = {token:idx for idx, token in enumerate(vocab)}\n",
    "tokens_to_idx['|unk|'] = len(tokens_to_idx)\n",
    "idx_to_token = {idx:token for token, idx in tokens_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_split_processing(tokens):\n",
    "    tokens_cleaned = [token.strip() for token in tokens if token.strip()]\n",
    "    final_tokens = []\n",
    "    i = 0\n",
    "    while i < (len(tokens_cleaned) - 1):\n",
    "        if tokens_cleaned[i] == '-' and tokens_cleaned[i+1] == '-':\n",
    "            if i+2 < len(tokens_cleaned) and tokens_cleaned[i+2] == '-':\n",
    "                final_tokens.append('---')\n",
    "                i = i+2\n",
    "            else:\n",
    "                final_tokens.append('--')\n",
    "                i = i+1\n",
    "        else:\n",
    "            final_tokens.append(tokens_cleaned[i])\n",
    "        i = i+1\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack']\n",
      "[54, 45, 150, 1013, 58]\n",
      "I HAD always thought Jack\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I HAD always thought Jack Gisburn' # Test Sentence\n",
    "final_split_expression= r'([,.:;?_\\(\\)\\\"\\']|-|--|---|\\s+|\\n)'\n",
    "sentence_tokens = post_split_processing(text_to_tokens(sentence, final_split_expression))\n",
    "print(sentence_tokens)\n",
    "sentence_tokens = [tokens_to_idx[word_token] for word_token in sentence_tokens] # Doesn't handle if token is not present\n",
    "print(sentence_tokens)\n",
    "decoded_sentence = ' '.join([idx_to_token[token_idx] for token_idx in sentence_tokens])\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Andrew']\n",
      "[69, 1140, 588, 1140]\n",
      "My |unk| is |unk|\n"
     ]
    }
   ],
   "source": [
    "sentence = 'My name is Andrew.' # Test Sentence\n",
    "final_split_expression= r'([,.:;?_\\(\\)\\\"\\']|-|--|---|\\s+|\\n)'\n",
    "sentence_tokens = post_split_processing(text_to_tokens(sentence, final_split_expression))\n",
    "print(sentence_tokens)\n",
    "sentence_tokens = [tokens_to_idx.get(word_token, tokens_to_idx['|unk|']) for word_token in sentence_tokens]\n",
    "print(sentence_tokens)\n",
    "decoded_sentence = ' '.join([idx_to_token[token_idx] for token_idx in sentence_tokens])\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying our tokenizer class functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Build Vocabulary ##########\n"
     ]
    }
   ],
   "source": [
    "from tokenizer.simple_tokenizer import RegexTokenizer\n",
    "split_regex= r'([,.:;?!_\\(\\)\\\"\\']|-|--|---|\\s+|\\n)'\n",
    "tokenizer = RegexTokenizer(raw_data, split_regex, special_tokens={'|unk|', }, unknown_token='|unk|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 1140, 588, 1140]\n",
      "My |unk| is |unk|\n"
     ]
    }
   ],
   "source": [
    "sentence = 'My name is Andrew.'\n",
    "encoded_sentence = tokenizer.encode(sentence)\n",
    "print(encoded_sentence)\n",
    "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "\n",
    "This tokenizer is subword tokenizer which involves encoding a text in its byte representation (typically done using `utf-8`). Following which, algorithm iteratively combines tokens of maximum frequency to form new tokens till a maximum vocabulary size is reached. It allows representation of many languages (via `utf-8`) and doesn't involve any heurestic based design mechanism for tokenization but depends entirely on the corpus of data available for combining tokens.\n",
    "\n",
    "Doesn't need special <span style=\"color:red\">unknown</span> token since it's a subword tokenizer and every character is present in vocabulary so it can break a word, not seen in training data, into corresponding character and tokenize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: (200) |||| I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a\n",
      "b'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a'\n",
      "Length: (200) |||| [73, 32, 72, 65, 68, 32, 97, 108, 119, 97, 121, 115, 32, 116, 104, 111, 117, 103, 104, 116, 32, 74, 97, 99, 107, 32, 71, 105, 115, 98, 117, 114, 110, 32, 114, 97, 116, 104, 101, 114, 32, 97, 32, 99, 104, 101, 97, 112, 32, 103, 101, 110, 105, 117, 115, 45, 45, 116, 104, 111, 117, 103, 104, 32, 97, 32, 103, 111, 111, 100, 32, 102, 101, 108, 108, 111, 119, 32, 101, 110, 111, 117, 103, 104, 45, 45, 115, 111, 32, 105, 116, 32, 119, 97, 115, 32, 110, 111, 32, 103, 114, 101, 97, 116, 32, 115, 117, 114, 112, 114, 105, 115, 101, 32, 116, 111, 32, 109, 101, 32, 116, 111, 32, 104, 101, 97, 114, 32, 116, 104, 97, 116, 44, 32, 105, 110, 32, 116, 104, 101, 32, 104, 101, 105, 103, 104, 116, 32, 111, 102, 32, 104, 105, 115, 32, 103, 108, 111, 114, 121, 44, 32, 104, 101, 32, 104, 97, 100, 32, 100, 114, 111, 112, 112, 101, 100, 32, 104, 105, 115, 32, 112, 97, 105, 110, 116, 105, 110, 103, 44, 32, 109, 97, 114, 114, 105, 101, 100, 32, 97]\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = raw_data[:200]\n",
    "print(f\"Length: ({len(sample_sentence)}) |||| {sample_sentence}\")\n",
    "byte_representation = sample_sentence.encode('utf-8')\n",
    "print(byte_representation)\n",
    "tokens = list(map(int, byte_representation))\n",
    "print(f\"Length: ({len(sample_sentence)}) |||| {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== TOKENS ====================\n",
      "[73, 32, 72, 65, 68, 32, 97, 108, 119, 97, 121, 115, 32, 116, 104, 111, 117, 103, 104, 116, 32, 74, 97, 99, 107, 32, 71, 105, 115, 98, 117, 114, 110, 32, 114, 97, 116, 104, 101, 114, 32, 97, 32, 99, 104, 101, 97, 112, 32, 103, 101, 110, 105, 117, 115, 45, 45, 116, 104, 111, 117, 103, 104, 32, 97, 32, 103, 111, 111, 100, 32, 102, 101, 108, 108, 111, 119, 32, 101, 110, 111, 117, 103, 104, 45, 45, 115, 111, 32, 105, 116, 32, 119, 97, 115, 32, 110, 111, 32, 103, 114, 101, 97, 116, 32, 115, 117, 114, 112, 114, 105, 115, 101, 32, 116, 111, 32, 109, 101, 32, 116, 111, 32, 104, 101, 97, 114, 32, 116, 104, 97, 116, 44, 32, 105, 110, 32, 116, 104, 101, 32, 104, 101, 105, 103, 104, 116, 32, 111, 102, 32, 104, 105, 115, 32, 103, 108, 111, 114, 121, 44, 32, 104, 101, 32, 104, 97, 100, 32, 100, 114, 111, 112, 112, 101, 100, 32, 104, 105, 115, 32, 112, 97, 105, 110, 116, 105, 110, 103, 44, 32, 109, 97, 114, 114, 105, 101, 100, 32, 97]\n",
      "Number of tokens before BPE: 200\n",
      "==================== NEW TOKENS ====================\n",
      "[299, 98, 271, 110, 32, 114, 272, 256, 114, 258, 32, 99, 273, 112, 262, 274, 105, 117, 115, 275, 116, 104, 264, 258, 262, 111, 111, 265, 102, 101, 108, 276, 119, 32, 274, 264, 275, 115, 266, 105, 260, 269, 115, 32, 110, 111, 262, 114, 101, 97, 260, 115, 271, 112, 114, 261, 278, 109, 278, 273, 114, 270, 272, 267, 268, 257, 279, 256, 105, 259, 260, 111, 102, 32, 280, 262, 276, 114, 121, 267, 279, 104, 97, 265, 100, 114, 111, 112, 112, 101, 265, 280, 32, 112, 97, 268, 116, 268, 103, 267, 109, 97, 114, 114, 105, 101, 100, 258]\n",
      "Number of tokens after BPE: 108\n",
      "Compression Ratio: 1.85x\n"
     ]
    }
   ],
   "source": [
    "def find_max_frequency_pair(token_ids):\n",
    "    char_counts = {}\n",
    "    for char1, char2 in zip(token_ids, token_ids[1:]):\n",
    "        char_counts[(char1, char2)] = char_counts.get((char1, char2), 0) + 1\n",
    "    \n",
    "    char_counts = max(char_counts.items(), key=lambda x: x[1])\n",
    "    return char_counts\n",
    "\n",
    "def combine_characters(token_ids, pair_combine, token_id):\n",
    "    idx = 0\n",
    "    new_tokens_list = []\n",
    "    while idx < (len(token_ids) - 1):\n",
    "        if (token_ids[idx] == pair_combine[0]) and (token_ids[idx+1] == pair_combine[1]):\n",
    "            new_tokens_list.append(token_id)\n",
    "            idx+=1\n",
    "        else:\n",
    "            new_tokens_list.append(token_ids[idx])\n",
    "        idx+=1\n",
    "    # If missed last token\n",
    "    if idx == len(token_ids) - 1:\n",
    "        new_tokens_list.append(token_ids[idx])\n",
    "    return new_tokens_list\n",
    "\n",
    "print(f\"{'='*20} TOKENS {'='*20}\")\n",
    "print(tokens)\n",
    "print(f\"Number of tokens before BPE: {len(tokens)}\")\n",
    "\n",
    "new_tokens = tokens[:]\n",
    "new_token_idx = 256\n",
    "token_to_idx = {}\n",
    "vocabulary_size = 300\n",
    "\n",
    "for i in range(vocabulary_size - 256):\n",
    "    max_freq_tokens = find_max_frequency_pair(new_tokens)[0]\n",
    "    # print(f'Merging {max_freq_tokens}')\n",
    "    new_tokens = combine_characters(new_tokens, max_freq_tokens, new_token_idx+i)\n",
    "    token_to_idx[max_freq_tokens] = new_token_idx+i\n",
    "print(f\"{'='*20} NEW TOKENS {'='*20}\")\n",
    "print(new_tokens)\n",
    "print(f\"Number of tokens after BPE: {len(new_tokens)}\")\n",
    "print(f\"Compression Ratio: {len(tokens)/len(new_tokens):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding/Decoding text to token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "for pair,idx in token_to_idx.items():\n",
    "    idx_to_vocab[idx] = idx_to_vocab[pair[0]] + idx_to_vocab[pair[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(token_ids):\n",
    "    tokens = b\"\".join(idx_to_vocab[idx] for idx in token_ids)\n",
    "    return tokens.decode('utf-8', errors='ignore')\n",
    "decode(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299, 98, 271, 110, 32, 114, 272, 256, 114, 258, 32, 99, 273, 112, 262, 274, 105, 117, 115, 275, 116, 104, 264, 258, 262, 111, 111, 265, 102, 101, 108, 276, 119, 32, 274, 264, 275, 115, 266, 105, 260, 269, 115, 32, 110, 111, 262, 114, 101, 97, 260, 115, 271, 112, 114, 261, 278, 109, 278, 273, 114, 270, 272, 267, 268, 257, 279, 256, 105, 259, 260, 111, 102, 32, 280, 262, 276, 114, 121, 267, 279, 104, 97, 265, 100, 114, 111, 112, 112, 101, 265, 280, 32, 112, 97, 268, 116, 268, 103, 267, 109, 97, 114, 114, 105, 101, 100, 258]\n",
      "Number of encoded tokens: 108\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a\n"
     ]
    }
   ],
   "source": [
    "def encode(token_ids):\n",
    "    new_tokens = list(token_ids)\n",
    "    for pair,token_idx in token_to_idx.items():\n",
    "        new_tokens = combine_characters(new_tokens, pair, token_idx)\n",
    "    return new_tokens\n",
    "encoded_tokens = encode(tokens)\n",
    "print(encoded_tokens)\n",
    "print(f\"Number of encoded tokens: {len(encoded_tokens)}\")\n",
    "print(decode(encoded_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying our tokenizer class functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer.byte_pair_tokenizer import BytePairTokenizer\n",
    "sample_sentence = raw_data[:200]\n",
    "vocabulary_size = 300 - 256\n",
    "bpe = BytePairTokenizer(sample_sentence, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299, 98, 271, 110, 32, 114, 272, 256, 114, 258, 32, 99, 273, 112, 262, 274, 105, 117, 115, 275, 116, 104, 264, 258, 262, 111, 111, 265, 102, 101, 108, 276, 119, 32, 274, 264, 275, 115, 266, 105, 260, 269, 115, 32, 110, 111, 262, 114, 101, 97, 260, 115, 271, 112, 114, 261, 278, 109, 278, 273, 114, 270, 272, 267, 268, 257, 279, 256, 105, 259, 260, 111, 102, 32, 280, 262, 276, 114, 121, 267, 279, 104, 97, 265, 100, 114, 111, 112, 112, 101, 265, 280, 32, 112, 97, 268, 116, 268, 103, 267, 109, 97, 114, 114, 105, 101, 100, 258]\n",
      "True\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a\n"
     ]
    }
   ],
   "source": [
    "print(bpe.encode(sample_sentence))\n",
    "print(bpe.encode(sample_sentence) == encoded_tokens)\n",
    "print(bpe.decode(bpe.encode(sample_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 164, 168, 224, 164, 174, 224, 164, 184, 224, 165, 141, 224, 164, 164, 224, 165, 135]\n",
      "Decoded word is: नमस्ते\n"
     ]
    }
   ],
   "source": [
    "print(bpe.encode('नमस्ते')) # Can encode things it has not seen during training\n",
    "encoding_hindi = bpe.encode('नमस्ते')\n",
    "print(f'Decoded word is: {bpe.decode(encoding_hindi)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
